---
title: è®ºæ–‡1ç®—æ³•
top: false
cover: false
toc: true
mathjax: false
date: 2022-03-30 14:02:24
author:
img:
coverImg:
password:
summary:
tags:
categories:
---

# é…åˆä¸€æœ¬å®ä½“ç¬”è®°æœ¬

# æ„å»ºåŒç½‘ç»œååŒä¼˜åŒ–çš„dataloader

## 20220330

- ä»æœ€åˆçš„ä¸€ä¸ªæ ·æœ¬é¢„æµ‹ä¸‹ä¸€ä¸ªæ ·æœ¬çš„æ¨¡å‹è¿™æ ·çš„ç¼ºç‚¹ï¼š
  - æ¯ä¸ªæ ·æœ¬æ›´æ–°ä¸€æ¬¡æ¢¯åº¦ï¼Œè®©ä¼˜åŒ–å™¨éå¸¸å—å•ä¸ªæ ·æœ¬çš„å·¦å³å¹²æ‰°ï¼Œä¸åˆ©äºæ”¶æ•›ï¼ŒåŒæ ·é€Ÿåº¦ä¼šå¾ˆæ…¢
- è§£å†³æ–¹æ¡ˆ
  - åœ¨å¼ºåŒ–å­¦ä¹ é¢„æµ‹ç½‘ç»œçš„éƒ¨åˆ†ï¼Œè¾“å…¥batchsizeä¸ªæ ·æœ¬çš„çŠ¶æ€å‘é‡ï¼Œè¾“å‡ºbatchsizeä¸ªæ ·æœ¬çš„æ•°æ®é•¿åº¦
  - æ‰§è¡Œæ–¹æ¡ˆä¸€ï¼š
    - å› ä¸ºå¼ºåŒ–å­¦ä¹ çš„batchsizeæ ·æœ¬æ˜¯åœ¨å­˜ä¸‹æ¥çš„ç»éªŒé‡Œå–åˆ°çš„ï¼Œæ‰€ä»¥ä¸éœ€è¦ç‰¹åˆ«åœ°å†æ„å»ºdataloaderï¼Œå°±æŠŠå›¾ç¥ç»ç½‘ç»œçš„è¾“å‡ºçš„ç‰¹å¾å‘é‡ä¸²è¡Œè¾“å…¥åˆ°å¼ºåŒ–å­¦ä¹ ç½‘ç»œ
    - å…·ä½“æ­¥éª¤
      1. å…ˆéšæœºç”Ÿæˆbatchsizeä¸ªæ•°æ®é•¿åº¦ï¼Œç„¶åæ•°æ®é¢„å¤‡å™¨å–å‡ºï¼Œé€å…¥å›¾ç½‘ç»œbatchsizeä¸ªæ•°æ®è¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼Œå¹¶æŠŠç”Ÿæˆçš„batchsizeä¸ªå·ç§¯åçš„ç‰¹å¾å‘é‡é€å…¥å¼ºåŒ–å­¦ä¹ ï¼Œä¾æ¬¡å¾ªç¯
  - æ‰§è¡Œæ–¹æ³•äºŒï¼š
    - æŠŠ



# æ¨¡å‹çš„ç­–ç•¥æ”¹è¿›

## 20220330

### å›¾ç¥ç»ç½‘ç»œé¢„è®­ç»ƒ

- ä½¿ç”¨batchsizeå¹¶ä¸”shuffleçš„æ¨¡å¼è¿›è¡Œè®­ç»ƒ



## å¼ºåŒ–å­¦ä¹ æ”¹è¿›

### 20220403

1. å¯ä»¥æŒ‰ä¸€å®šçš„æ¦‚ç‡éšæœºé€‰æ‹©åŠ¨ä½œï¼Œå¢åŠ çµæ´»æ€§
2. updataçš„æ¬¡æ•°å¯ä»¥é€‚å½“å¢åŠ 
3. å¦‚æœåŒæ—¶å‡ ä¸ªactionçš„åˆ†æ•°å€¼æ˜¯ç›¸ç­‰çš„ï¼Œä¸åŠ ä¿®æ”¹ç¨‹åºæ¯æ¬¡éƒ½é€‰æ‹©ç¬¬ä¸€ä¸ªåŠ¨ä½œï¼Œå¯ä»¥ä¿®æ”¹actionçš„ä½ç½®æ¥è°ƒæ•´
4.   å¯¹rewardæ­£åˆ™åŒ– å˜ä¸º-1 ï½ 1æˆ–å…¶ä»–
5. Prioritized æŒ‰é‡è¦åº¦ç¨‹åº¦æŠ½æ ·  
6. å½“rewardé«˜äºbaselineæ—¶å¢åŠ é‡‡æ ·å‡ ç‡ï¼Œå¦åˆ™å‡å°‘
7. actionæ€»æ˜¯ä¸å˜éœ€è¦æ‰£åˆ†ï¼Œactionå˜åŒ–æ˜¯åŠ åˆ†é¡¹
8. æ ¹æ®domain knowledge è®¾è®¡æ›´å¤šçš„ reward æœºåˆ¶ï¼Œå¢åŠ çµæ´»æ€§
9. å¢åŠ æœºå™¨çš„å¥½å¥‡å¿ƒ

## reward æ”¹è¿›

- å¦‚æœé”™çš„å¤šéœ€è¦åŠ å¤§æƒ©ç½š

### 20220409

- è€ç‰ˆæœ¬ Alpha Goä¸€å¼€å§‹ä½¿ç”¨çš„æ˜¯Behavior cloning 

# å›¾æ•°æ®å¯è§†åŒ–

## éœ€æ±‚

1. æ ¹æ®å‡ºå…¥åº¦çš„å¤§å°è°ƒæ•´èŠ‚ç‚¹çš„å¤§å°
2. æ ¹æ®è¾¹æƒé‡çš„å¤§å°è°ƒæ•´è¾¹çš„ç²—ç»†
3. å¯ä»¥å¾—åˆ°æœ‰å‘å›¾çš„æŒ‡å‘ä¿¡æ¯
4. ä¸åŒæ ‡ç­¾çš„èŠ‚ç‚¹æ ‡æ³¨ä¸åŒçš„é¢œè‰²
4. æŠŠstate normalizeåˆ°ä¸€ä¸ªå°ºåº¦



# é—ä¼ ç®—æ³•éƒ¨åˆ†

## å›¾ç½‘ç»œæ¨¡å‹æœç´¢

### æœç´¢ç©ºé—´

- [ ] 2022-04-20 14:23:02 æ˜¯å¦æŠŠ add-selfåŠ å…¥æœç´¢ç©ºé—´  ï¼ˆç›®å‰æ²¡æœ‰ï¼‰
- [ ] 2022-04-20 14:42:08 æ˜¯å¦æŠŠ å›¾å·ç§¯å±‚å†…çš„ concat ä½œä¸ºæœç´¢ç©ºé—´çš„ä¸€éƒ¨åˆ† ï¼ˆç›®å‰æ²¡æœ‰ï¼‰
- [ ] 2022-04-29 21:32:32 æ˜¯å¦æŠŠ å›¾å·ç§¯çš„æ¯ä¸€å±‚çš„éšè—å±‚ä½œä¸ºæœç´¢ç©ºé—´çš„ä¸€éƒ¨åˆ† ï¼ˆç›®å‰æ²¡æœ‰ï¼‰
- [ ] 2022-05-01 16:03:17 æ˜¯å¦æŠŠ è®­ç»ƒçš„batchsizeä½œä¸ºæœç´¢ç©ºé—´æœç´¢ ï¼ˆç›®å‰æ²¡æœ‰ï¼‰



# å·ç§¯éƒ¨åˆ†

- ç”±äºå›¾åƒè¾¹ç•Œç‚¹å·ç§¯ï¼ˆè¾¹ç•Œç‚¹å‚ä¸çš„è®¡ç®—æ¬¡æ•°å°‘ï¼‰é—®é¢˜ï¼Œæ˜¯å¦éœ€è¦åœ¨ç”Ÿæˆçš„01äºŒè¿›åˆ¶å›¾å·ç§¯æ—¶å¯¹äºŒè¿›åˆ¶å›¾åƒè¿›è¡Œpadding
- å¯å¦å¯ä»¥åˆå¹¶å‡ ä½æ•°æ®åšåŠ å’Œåœ¨å½’ä¸€åŒ–ç„¶ååšå…¥ä¾µè¯†åˆ«
- å·ç§¯å®éªŒå¯ä»¥å¯¹æ¯”ä¸€ä¸‹VGG
- 2022-08-12 21:04:45 **æŠŠæ¯ä¸ªæŠ¥æ–‡ä½¿ç”¨ä¸€ä¸ªå‘é‡æ¥è¡¨ç¤º**



# å¼ºåŒ–å­¦ä¹ 

- å¢åŠ çŠ¶æ€è¡¨ç¤ºçš„ä¿¡æ¯
- å¯è§†åŒ–è¾“å…¥åˆ°å¼ºåŒ–å­¦ä¹ çš„å‘é‡ï¼Œçœ‹æ˜¯å¦å‘ç”Ÿäº†æ˜æ˜¾çš„å˜åŒ–
- å½“å¥–åŠ±ç´¯è®¡åˆ°ä¸€å®šçš„è´Ÿå€¼åˆ™é‡æ–°å¼€å§‹
- Reward å½’ä¸€åŒ–äº†

## çŸ¥è¯†å½’çº³

- PPOçš„å‰èº«æ˜¯TRPO



## å±€é™

- æ— æ³•åº”å¯¹æ¯”è¾ƒæ–°çš„æ¨¡å¼ï¼Œå¯ä»¥ç©å½“å‰æ¸¸æˆï¼Œæ— æ³•é€‚åº”æ–°æ¸¸æˆ--



# æ•°æ®é›†éƒ¨åˆ†

## æ•°æ®é›†å‘½å

### Car_Hacking_Challenge_Dataset_rev20Mar2021

```
datadir='../data/Car_Hacking_Challenge_Dataset_rev20Mar2021/0_Preliminary/
```

- MergeTrainSub_D
  - MergeTrainSub_D_0_1_2_sub_dataset
    - MergeTrainSub_D_0_1_2_sub_random_1_200_500_A
  - MergeTrainSub_D_0_1_2_sub_processed
    - MergeTrainSub_D_0_1_2_sub_processed/ps_10_nor_true_random_42/dataset_Di_true_200_500.p
    - MergeTrainSub_D_0_1_2_sub_processed/ps_10_nor_true_random_42/graphs_list_Di_200_500.p



# ç¨‹åºå¹¶è¡Œè¿è¡Œéƒ¨åˆ†

100ä¸ªä¸ªä½“åˆ†ä¸º4ä»½åŒæ—¶è¿è¡Œï¼Œæ¯ä»½ä¸²è¡Œæ‰§è¡Œ25

æ¯ä¸ªä¸ªä½“è¿è¡Œçš„logä¿¡æ¯é‡å®šå‘åˆ°å„è‡ªlogæ–‡ä»¶



# ç¨‹åºPymooé—ä¼ éƒ¨åˆ†

- æœç´¢ç©ºé—´å…±19ä½

  | åŸºå› ä½æ•° | åŸºå› ä½æ•° | æœ€å°å€¼ | æœ€å¤§å€¼ |                 åŸºå› å«ä¹‰è§£é‡Š                 |
  | :------: | :------: | :----: | :----: | :------------------------------------------: |
  |    0     |    1     |   0    |   1    |                 æ˜¯å¦ä¸ºæœ‰å‘å›¾                 |
  |    1     |    1     |   0    |   1    |          å›¾åç¼©è¿‡ç¨‹ä¸­æ˜¯å¦ä½¿ç”¨æ­£åˆ™åŒ–          |
  |   2-3    |    2     |   0    |   2    |       ä¸¤ä¸ªï¼ˆå›¾åç¼©å‰åï¼‰å›¾å·ç§¯å—çš„æ·±åº¦       |
  |   4-5    |    2     |   0    |   2    |           é¢„æµ‹å±‚ä¸¤ä¸ªå…¨è¿æ¥å±‚çš„æ¯”ä¾‹           |
  |    6     |    1     |   0    |   5    |             ç¥ç»ç½‘ç»œDropoutå‚æ•°              |
  |    7     |    1     |   0    |   3    |           ç¥ç»ç½‘ç»œWeightDecayå‚æ•°            |
  |    8     |    1     |   0    |   3    |                ç¥ç»ç½‘ç»œå­¦ä¹ ç‡                |
  |   9-10   |    2     |   0    |   2    | æ¯ä¸ªå›¾å·ç§¯å—å†…æ¯æ¬¡å·ç§¯è¾“å‡ºç‰¹å¾å‘é‡çš„ç»“åˆæ–¹å¼ |
  |  11-16   |    6     |   0    |   4    |        æ¯ä¸ªå›¾ç½‘ç»œè¾“å‡ºåç»è¿‡çš„æ¿€æ´»å‡½æ•°        |
  |  17-18   |    2     |   0    |   2    |       ä¸¤ä¸ªå¯å˜è¿æ¥å±‚è¾“å‡ºç»è¿‡çš„æ¿€æ´»å‡½æ•°       |
  |   åˆè®¡   |    19    |        |        |                                              |

  

  



# ç›®æ ‡å‡½æ•°å€¼

- params 
  - æœ€å¤§ï¼š279862
  - æœ€å°ï¼š22492

- flops
  - æœ€å¤§: 113016384
  - æœ€å°: 54379584

# ç¨‹åºå®è¡Œæ­¥éª¤

- å›¾ç½‘ç»œéƒ¨åˆ†
  - å…ˆè®­ç»ƒå‡ºè¶…ç½‘ç»œçš„å‚æ•°
  - ä½¿ç”¨è®­ç»ƒå‡ºçš„ä¸ªä½“è®­ç»ƒmlp
  - åœ¨è®­ç»ƒmlpä¸ªä½“ä¸­é€‰å‡ºä¼˜è‰¯ä¸ªä½“ï¼Œä½œä¸ºåˆå§‹ä»£
  - æ¯ä»£ä¸ªä½“ æ•°é‡ 60
  - æ¯ä¸ªä¸ªä½“è®­ç»ƒ 20 epoch



# 0510ä»»åŠ¡

- ä¿®æ”¹ å®æ—¶è·å–æ•°æ®ç¨‹åº ä¸éœ€è¦shuffle å¹¶ä¸”åŠ ä¸Š D_1_2_sub
- åœ¨ç¨‹åºç»“å°¾éªŒè¯æ•°æ®å¯¹é½ ç¨‹åºç»“æŸæ—¶æŸ¥çœ‹ graphå‰©ä¸‹çš„æ•°æ®å¤šå°‘ conv_canæ•°æ®é›†å‰©ä¸‹å¤šå°‘ï¼Œå¯¹æ¯”ä¸€ä¸‹
- ä¸€ä¸ªä¸€ä¸ªbatchçš„é¢„æµ‹ å› ä¸ºæœ€ågraphå½¢ä¸æˆä¸€ä¸ªbatch å°±ä¼šç»“æŸ ä¼šæµªè´¹æ•°æ®ï¼Œå…ˆä¸ç®¡è¿™ä¹ˆå¤šäº†
- é‡è¦ä»»åŠ¡ï¼Œ**è·å–ä¸¤ä¸ªç½‘ç»œç»“åˆä¹‹åçš„å…¥ä¾µæ£€æµ‹ç²¾åº¦**
- ä¸»è¦éœ€è¦è§£å†³å’ŒéªŒè¯**ä¸¤ä¸ªæ•°æ®é›†çš„å¯¹é½é—®é¢˜**
- 

# å·®å‡ å¼ å›¾

- å›¾ç¥ç»ç½‘ç»œçš„æ€»è§ˆå›¾
- å›¾ç¥ç»ç½‘ç»œçš„é—ä¼ ä¼˜åŒ–å›¾
- å·ç§¯ç½‘ç»œçš„é—ä¼ ä¼˜åŒ–å›¾



# ç»“æœéƒ¨åˆ†

- CNNæ¨¡å‹ å†™æˆåŸºå› çš„å½¢å¼ 444 4 0 444 å¤æ‚åº¦ 2628714486
- LSTMæ¨¡å‹ å¤æ‚åº¦ 57344
- DNN acc: 0.9665764762056342 å¤æ‚åº¦ 6817762
- GNN å¤æ‚åº¦ 54379585
- æœ¬æ¨¡å‹ GNN+CNN 2683094071
- å…¨è¿æ¥ç½‘ç»œæ¨¡å‹
- ä¼˜åŒ–çš„å•ç‹¬çš„CNN
- ä¼˜åŒ–çš„å•ç‹¬çš„GNN



- äº”ä¸ªæ¨¡å‹çš„è®­ç»ƒæ›²çº¿ æœ‰æ— æ–¹å‘ å’Œ æœ‰æ— æ­£åˆ™åŒ–çš„ ä¸¤ç§å›¾çº¿å¯¹æ¯”
- ä¸¤ä¸ªæ¨¡å—çš„é—ä¼ ç®—æ³•éƒ¨åˆ†çš„ç»“æœ
- è¡¨æ ¼å±•ç¤ºç»“æœ



# LSTM

- æŠŠD_1ä¸­çš„ IDçš„ 220 éƒ¨åˆ† è½¬æ¢ä¸ºæ•°æ®é›†
- ç»“æœ
  - ä½¿ç”¨ID220 D0è®­ç»ƒ D1éªŒè¯ ç²¾åº¦ 97% 0.9701388004584235



# DNN

- è¿™äº›åŠŸèƒ½çš„æ‰§è¡Œæ•ˆç‡é«˜ã€å¤æ‚åº¦ä½ï¼Œå› ä¸ºå®ƒä»¬æ˜¯é€šè¿‡ç½‘ç»œç›´æ¥ä»æ¯”ç‰¹æµç”Ÿæˆçš„ã€‚è¯¥æŠ€æœ¯åœ¨ç¦»çº¿è®­ç»ƒç‰¹å¾çš„åŒæ—¶ç›‘æµ‹è½¦è¾†ç½‘ç»œä¸­çš„äº¤æ¢æ•°æ®åŒ…ï¼Œå¹¶åœ¨æˆ‘ä»¬çš„å®éªŒä¸­æä¾›äº†å¯¹æ”»å‡»çš„å®æ—¶å“åº”ï¼Œå…·æœ‰æ˜¾è‘—çš„é«˜æ£€æµ‹ç‡ã€‚
- ä»Šå¤©çš„è½¦è¾†ç³»ç»ŸåµŒå…¥äº†è®¸å¤šç§°ä¸ºç”µå­æ§åˆ¶å•å…ƒï¼ˆECUï¼‰çš„è®¡ç®—æœºè®¾å¤‡æ¥æ§åˆ¶å’Œç›‘æ§å­ç³»ç»Ÿã€1ã€‘ã€‚ECUé€šè¿‡è½¦å†…ç½‘ç»œç›¸äº’é€šä¿¡ï¼Œä»¥ä¿ƒè¿›å…ˆè¿›çš„æ±½è½¦åº”ç”¨ï¼Œå¦‚carputerå’Œè‡ªåŠ¨é©¾é©¶æœåŠ¡ã€‚æ§åˆ¶å™¨å±€åŸŸç½‘ï¼ˆCANï¼‰[2]æä¾›äº†ä¸€ç§ç®€å•å¯é çš„é€šä¿¡åè®®ï¼Œä½œä¸ºè½¦è¾†ç½‘ç»œçš„è®¾è®¡æ ‡å‡†ï¼Œä¸ä»…è¿æ¥ä¼ æ„Ÿå™¨å’Œæ§åˆ¶å™¨ï¼Œè€Œä¸”è¿æ¥äº’è”ç½‘ã€‚éšç€è½¦å¯¹è½¦ï¼ˆV2Vï¼‰å’Œè½¦å¯¹åŸºç¡€è®¾æ–½ï¼ˆV2Iï¼‰é€šä¿¡æ¥å£çš„å‡ºç°ï¼ŒCANçš„é‡‡ç”¨åŠ å¿«äº†åº”ç”¨ç¨‹åºçš„é€Ÿåº¦[3]ã€‚ç„¶è€Œï¼Œè½¦è¾†ç³»ç»Ÿçš„å¼€æ”¾æ€§å¢åŠ äº†æ¶æ„ç½‘ç»œæ”»å‡»çš„é£é™©ï¼Œè¿™äº›æ”»å‡»å¯èƒ½ä¼šä¸¥é‡æŸå®³äººçš„ç”Ÿå‘½ã€‚è½¦è¾†ç½‘ç»œä¸­çš„ä¿æŠ¤æœºåˆ¶å…³æ³¨æ–°çš„å®‰å…¨å¨èƒã€‚



# å·ç§¯é—ä¼ ç®—æ³•éƒ¨åˆ†

- äº¤å‰è¿‡åï¼Œæœ‰é‡å¤ä¸ªä½“

- 2022-08-11 23:17:08 æœç´¢æŸå¤±å‡½æ•°



# å…³äºå†™ä½œæ—¶çš„é—®é¢˜

1. è®ºæ–‡ä¸­åªæœ‰å›¾åç¼©éƒ¨åˆ†å‡ºç°åç¼©è¿‡ç¨‹çš„å…¬å¼ï¼Œå› ä¸ºåªæœ‰è¿™é‡Œæœ‰å…¬å¼æ‰€ä»¥å°±ä¿ç•™ç€äº†ï¼Œä½†æ˜¯è¿™é‡Œåç¼©çš„è¿‡ç¨‹æ˜¯å‚è€ƒçš„ä¹‹å‰çš„è®ºæ–‡

   Y. Ma, S. Wang, C. C. Aggarwal, and J. Tang, "Graph Convolutional Networks with EigenPooling," presented at the Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, Anchorage, AK, USA, 2019. [Online]. Available: https://doi.org/10.1145/3292500.3330982.

   æˆ‘æŠŠè¿™ç¯‡è®ºæ–‡ä¸­çš„æ–¹æ³•è¿ç”¨åˆ°äº†æ„é€ çš„æŠ¥æ–‡å›¾æ•°æ®çš„å›¾åç¼©åˆ†ç±»è¿‡ç¨‹ã€‚

   æˆ‘çš„é—®é¢˜æ˜¯æ˜¯å¦å¯ä»¥ä¿ç•™è¿™é‡Œçš„å…¬å¼ï¼Ÿ

2. è®ºæ–‡çš„





# Intro

## NAS

- 

## å›¾ç¥ç»ç½‘ç»œ

- é—ä¼ ç®—æ³•ä¼˜åŒ–å›¾ç½‘ç»œ
  æœ€è¿‘ï¼Œç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰å¸å¼•äº†è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶å…´è¶£[2]ã€‚å¦‚å›¾1æ‰€ç¤ºï¼ŒGNNsä½“ç³»ç»“æ„ä¸­æœ‰è®¸å¤šç»„ä»¶ï¼Œå¦‚æ³¨æ„ã€èšåˆå’Œæ¿€æ´»åŠŸèƒ½ï¼Œå®ƒä»¬æ„æˆäº†å›¾å½¢ç¥ç»ç½‘ç»œçš„æœç´¢ç©ºé—´ã€‚æ¶æ„æœç´¢ç®—æ³•ä»æœç´¢ç©ºé—´ä¸­å¯¹ç»„ä»¶çš„ç»„åˆè¿›è¡Œé‡‡æ ·ğ‘† ä¾‹å¦‚{ğ‘” ğ‘ğ‘¡,ğ‘ ğ‘¢ğ‘š,ğ‘¡ğ‘ğ‘›â„, ...} åœ¨æœ¬ä¾‹ä¸­ï¼Œä½œä¸ºGNNsä½“ç³»ç»“æ„ğ‘ . æ¶æ„ï¼ˆarchitectureï¼‰ğ‘  é€šè¿‡æ€§èƒ½è¯„ä¼°ç­–ç•¥è¿›ä¸€æ­¥è¯„ä¼°ã€‚æœç´¢ç®—æ³•ä½¿ç”¨ä¼°è®¡çš„æ€§èƒ½ä»æœç´¢ç©ºé—´ç”Ÿæˆæ–°çš„æ›´å¥½çš„GNNsä½“ç³»ç»“æ„ğ‘†. æœ€è¿‘ï¼Œä¸€äº›ç ”ç©¶é›†ä¸­åœ¨å›¾ç¥ç»ç»“æ„æœç´¢é—®é¢˜ä¸Šã€‚





# 2022-08-18 ä¿®æ”¹è®ºæ–‡

- ç›®å‰è®ºæ–‡ä¸­ä½¿ç”¨åˆ°çš„æ¼”åŒ–ç®—æ³•éƒ¨åˆ†æ˜¯åœ¨ç¥ç»æ¶æ„æœç´¢ä¸Šï¼Œå·ç§¯ç½‘ç»œï¼Œå›¾ç¥ç»ç½‘ç»œæ¶æ„æœç´¢
- å¢åŠ å…¶ä»–çš„æ¼”åŒ–ç®—æ³•åº”ç”¨åœºæ™¯ï¼Œå¹¶åŠ å…¥æ”¹è¿›æ€æƒ³
- æ¼”åŒ–ç®—æ³•æ˜¯ä¸€ç§ä¼˜åŒ–ç®—æ³•ï¼Œæ±‚æœ€ä¼˜çš„ç®—æ³•
- æˆ‘çš„ç›®çš„æ˜¯å¾—åˆ°æœ€ä¼˜çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæ¯æ¬¡é‡‡æ ·çš„æœ€ä¼˜æ•°æ®é•¿åº¦ï¼ˆå¼ºåŒ–å­¦ä¹ è§£å†³ï¼‰
- åœ¨ä¼˜åŒ–ç¥ç»æ¶æ„æ–¹é¢æå‡ºåˆ›æ–°ç‚¹

## åŠ å…¥æ¼”åŒ–ç®—æ³•åˆ›æ–°å¢åŠ å…¥ä¾µæ£€æµ‹ç‡å’Œé™ä½æ¨¡å‹å¤æ‚åº¦

- ç›®å‰å¯ä»¥ç›´æ¥å…¥æ‰‹çš„æ”¹è¿›æ˜¯æå‡å…¥ä¾µè¯†åˆ«ç²¾åº¦ï¼ˆï¼‰---é™ä½æ¨¡å‹å¤æ‚åº¦ï¼ˆï¼‰
- æ›´å¥½çš„æå–å›¾ç‰¹å¾å’ŒäºŒè¿›åˆ¶å›¾åƒç‰¹å¾
- æ¼”åŒ–ç®—æ³•ä¸­çš„åŠ¨æ€ç¯å¢ƒå¦‚ä½•è€ƒè™‘



## åˆå§‹åŒ–å­ç§ç¾¤çš„åˆ›æ–°

- åŸºäºå¤šä¸ªä¸­å¿ƒç‚¹åˆå§‹åŒ–å­ç§ç¾¤




# ä»‹ç»éƒ¨åˆ†

- å¤§ä½“ä»‹ç»å›¾ç½‘ç»œçš„åº”ç”¨å’ŒåŸç†





# (å¤šç›®æ ‡æ¼”åŒ–å›¾ç¥ç»)è®ºæ–‡å†…å®¹å’Œå‚è€ƒ

1. æœç´¢ç©ºé—´å‚è€ƒ 2020-Neural Architecture Search in Graph Neural Networks
2. æ·»åŠ è·³è·ƒè¿æ¥  2020-AutoGraph Automated Graph Neural Network





# TEVCå‚è€ƒæ–‡çŒ®é˜…è¯»
## 2022-Evolutionary Search for Complete Neural Network Architectures With Partial Weight Sharing
### å¥½æƒ³æ³•+å¥½è¡¨è¾¾
1. However, the full weight sharing training paradigm in OSNAS may result in strong interference across candidate architectures and mislead the architecture search.
2. The efï¬ciency of performance estimation of the candidate architectures is therefore greatly enhanced because it avoids training a large number of candidate models from scratch.
   oneshotå¯ä»¥ä¸ç”¨åå¤è®­ç»ƒçš„ä¼˜åŠ¿
### å¥½è¡¨è¾¾
1. As a result, optimal networks obtained by Evo-OSNAS can achieve more competitive performance than the state-of-the-art networks found by existing NAS algorithms.
   å¤¸èµè‡ªå·±ç®—æ³•
2. NAS can be viewed as a bilevel optimization problem and mathematically formulated as follows: train valid
   ä»‹ç»NAS
3. RL-based NAS methods adopt a controller to sample a new candidate network to be trained and its performance is used as the reward score. Then, the reward score can be adopted to update the controller for sampling a better candidate network in the next iteration.
   ä»‹ç»å¼ºåŒ–å­¦ä¹ çš„NAS
4. The  benefit of the proposed mutation operation is to make the  one-shot model training less likely to get trapped in local minimums.
   å¤¸èµå˜å¼‚
### è§‚ç‚¹æå–
1. oneshotæ¨¡å‹è®­ç»ƒï¼Œè®¾ç½®å¼€å…³åŸºå› æ¥å†³å®šæ¯ä¸ªcellæ˜¯å¦å‚ä¸è®­ç»ƒï¼ˆå¯å‚è€ƒï¼‰
### æ€»ç»“
1. è™½ç„¶æœ¬æ–‡åº”ç”¨äº†oneshotï¼Œæœ¬æ–‡çš„æœç´¢ç©ºé—´æ¶‰åŠä¸åŒçš„æ“ä½œç±»å‹ï¼Œ<mark>æ¯ä¸€å±‚çš„éƒ½å›ºå®šä¸€ç§ç±»å‹çš„çš„æ“ä½œï¼Œæ¯æ¬¡é€‰æ‹©æ—¶é€‰ä¸­ï¼Œåˆ™ç›´æ¥ç»§æ‰¿ï¼Œä»è€Œå®ç°oneshotï¼Œä½†æ˜¯ä¸€å¼€å§‹æ— æ³•è®­ç»ƒæ‰€æœ‰æƒ…å†µçš„æ¨¡å—ï¼Œåˆ™éœ€è¦è®¾è®¡ä¸€ä¸ªæ¨¡å—åº“ã€‚äº¤å‰æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸¤é˜¶æ®µï¼ŒGNNå±‚é¢å’Œlayerå±‚é¢ã€‚ç„¶åå˜å¼‚æ ¹æ®åº“å†…çš„æƒ…å†µï¼Œå¢åŠ ä¸ªä½“å¤šæ ·æ€§ã€‚</mark>


### æ€»ç»“
1. oneshotç½‘ç»œ
2. æœ‰ç»“æ„åŸºå› å’ŒswichåŸºå› ä¸¤ç§
3. æ¼”åŒ–éƒ¨åˆ†æ˜¯äº¤å‰æ˜¯åˆ†åˆ«äº¤å‰ä¸¤ç§åŸºå› ç„¶åswitchåŸºå› ç›´æ¥å®Œæ•´ç»§æ‰¿ä¸€ä¸ªäº²æœ¬
4. ä½¿ç”¨äº†ç¦»æ•£å¤šé¡¹å¼å˜å¼‚


## 2021-Evolutionary Neural Architecture Search for High-Dimensional Skip-Connection Structures on DenseNet Style Networks
### å¥½è¡¨è¾¾
1. However, the use of neural architecture search for the discovery of skip-connection structures, an important element in modern convolutional neural networks, is limited within the literature.
   è¯´æ˜æŸé¡¹ç ”ç©¶å°‘
2. Genetic CNN was highly computationally expensive.
   æŸé¡¹ç ”ç©¶å¤æ‚åº¦é«˜
3. how these networks are represented within the evolutionary NAS algorithm.
   è¿™äº›ç½‘ç»œå¦‚ä½•åœ¨æ¼”åŒ–NASä¸­è¡¨ç¤º

## 2021-Multiobjective Evolutionary Design of Deep  Convolutional Neural Networks for Image  Classification-å¤šç›®æ ‡æ¼”åŒ–
### å¥½æƒ³æ³•+å¥½è¡¨è¾¾
1. 1) the obtained architectures are either  solely optimized for classification performance, or only for one  deployment scenario and 2) the search process requires vast  computational resources in most approaches.
   1ï¼‰ç›®å‰è·å¾—çš„æ¶æ„ä»…ä»…ä¸ºäº†åˆ†ç±»ç²¾åº¦ï¼Œæˆ–è€…ä»…ä»…åªæ˜¯ä¸€ç§éƒ¨ç½²åœºæ™¯ï¼Œ2ï¼‰æœç´¢è¿‡ç¨‹åœ¨å¤§éƒ¨åˆ†æ–¹æ³•ä¸­éƒ½éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æº
2. The flexibility provided from simultaneously obtaining multiple architecture choices for different compute requirements further differentiates our approach from other methods in the literature.
   åŒæ—¶è·å¾—ä¸åŒè®¡ç®—è¦æ±‚çš„å¤šä¸ªæ¶æ„é€‰æ‹©æ‰€æä¾›çš„çµæ´»æ€§**è¿›ä¸€æ­¥å°†æˆ‘ä»¬çš„æ–¹æ³•ä¸æ–‡çŒ®ä¸­çš„å…¶ä»–æ–¹æ³•åŒºåˆ†å¼€æ¥ã€‚**

### å¥½è¡¨è¾¾
1. The proposed method addresses the first shortcoming by populating a set of architectures to approximate the entire Pareto  frontier through genetic operations that recombine and modify  architectural components progressively.
   æå‡ºçš„æ–¹æ³•é€šè¿‡é—ä¼ æ“ä½œç”Ÿæˆä¸€ç³»åˆ—æ¥è¿‘å¸•ç´¯æ‰˜å‰æ²¿çš„æ¶æ„ï¼Œè¿™ç§é—ä¼ ç®—æ³•é€æ­¥é‡ç»„å’Œä¿®æ”¹æ¶æ„ç»„ä»¶
2. Our approach improves  computational efficiency by carefully down-scaling the architectures during the search as well as reinforcing the patterns  commonly shared among past successful architectures through  Bayesian model learning.
   æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åœ¨æœç´¢è¿‡ç¨‹ä¸­ä»”ç»†ç¼©å°æ¶æ„ï¼Œå¹¶é€šè¿‡è´å¶æ–¯æ¨¡å‹å­¦ä¹ åŠ å¼ºè¿‡å»æˆåŠŸæ¶æ„ä¹‹é—´é€šå¸¸å…±äº«çš„æ¨¡å¼æ¥æé«˜è®¡ç®—æ•ˆç‡ã€‚
3.  One of the main driving forces behind this success is the introduction of many CNN architectures, including GoogLeNet [1], ResNet [2], DenseNet [3], etc., in the context of object classification. Concurrently, architecture designs,such as ShuffleNet [4], MobileNet [5], LBCNN [6], etc., have been developed with the goal of enabling real-world deployment of high-performance models on resource-constrained devices. These developments are the fruits of years of painstaking efforts and human ingenuity.
   è¿™ä¸€æˆåŠŸèƒŒåçš„ä¸»è¦é©±åŠ¨åŠ›ä¹‹ä¸€æ˜¯åœ¨å¯¹è±¡åˆ†ç±»çš„èƒŒæ™¯ä¸‹å¼•å…¥äº†è®¸å¤šCNNæ¶æ„ï¼ŒåŒ…æ‹¬GoogLeNet [1]ï¼ŒResNet [2]ï¼ŒDenseNet [3]ç­‰ã€‚åŒæ—¶ï¼Œæ¶æ„è®¾è®¡ï¼Œå¦‚ShuffleNet [4]ï¼ŒMobileNet [5]ï¼ŒLBCNN [6]ç­‰ï¼Œå·²ç»å¼€å‘å‡ºæ¥ï¼Œç›®æ ‡æ˜¯åœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šå®ç°é«˜æ€§èƒ½æ¨¡å‹çš„å®é™…éƒ¨ç½²ã€‚è¿™äº›å‘å±•æ˜¯å¤šå¹´è‰°è‹¦åŠªåŠ›å’Œäººç±»èªæ˜æ‰æ™ºçš„æˆæœã€‚
4. Among the many different NAS methods being continually proposed, evolutionary algorithms (EAs) are getting a  plethora of attention, due to their population-based nature and  flexibility in encoding.ï¼ˆ**å¤¸èµæ¼”åŒ–ç®—æ³•**ï¼‰
   åœ¨ä¸æ–­æå‡ºçš„è®¸å¤šä¸åŒçš„ NAS æ–¹æ³•ä¸­ï¼Œè¿›åŒ–ç®—æ³• ï¼ˆEAï¼‰ å› å…¶åŸºäºç¾¤ä½“çš„æ€§è´¨å’Œç¼–ç çš„çµæ´»æ€§è€Œå—åˆ°å¤§é‡å…³æ³¨ã€‚
5. down-scale the architectures to create their proxy  models [8], [17], which can be optimized efficiently in the  lower-level through SGD.
   å¤¸èµä½¿ç”¨ä»£ç†çš„é‡è¦æ€§
### æ€»ç»“
1. é—ä¼ éƒ¨åˆ†çš„äº®ç‚¹æ˜¯ä½¿ç”¨äº†è´å¶æ–¯æ¨¡å‹è®¡ç®—å†å²ä¸­è¡¨ç°å¥½çš„æ¨¡å‹ä¸€äº›èŠ‚ç‚¹é¡ºåºä¹‹é—´çš„å…³ç³»ï¼Œæ¥åˆ©ç”¨



## 2022-GNN-EA: Graph Neural Network with Evolutionary  Algorithm
### å¥½è¡¨è¾¾
1. Unfortunately, existing graph NAS methods are usually susceptible to unscalable depth, redundant omputation, constrained search space and some other limitations.
   è¯´æ˜å½“å‰å›¾ç¥ç»ç½‘ç»œæ¶æ„æœç´¢çš„é™åˆ¶
2. The experiment results show that GNN-EA exhibits comparable performance to the previous state-of-the-art handcrafted and automated GNN models.
   è¡¨è¾¾è‡ªå·±çš„å›¾æœç´¢çš„ç»“æœéå¸¸å¥½
3. We present an evolutionary graph neural network architecture search strategy based on fine-grained atomic operations.
   è¡¨è¾¾å‡ºè‡ªå·±æå‡ºäº†ä¸€ä¸ªåŸºäºä»€ä¹ˆçš„å›¾ç½‘ç»œæœç´¢æ¡†æ¶
4. Following the search strategy, we propose GNNEA, a framework that can achieve adaptive adjustment of neural structures without human intervention.
   è¿™ä¸ªæœç´¢ç­–ç•¥è¿˜ä¸éœ€è¦äººçš„å¹²é¢„ï¼Œç»“æœå¾ˆå¥½
5. We conduct comparison experiments on five real-world datasets to evaluate our method. The results prove that GNN-EA outperforms the previous handcrafted GNN models and shows comparable performance to the state-of-the-art automated GNN models.
   æˆ‘ä»¬åšäº†å®éªŒï¼Œä¹Ÿå†™ä¸ºäº†è´¡çŒ®ä¹‹ä¸€
4. Table I shows the candidate set of atomic operations.
   ä»‹ç»æœç´¢ç©ºé—´
5. The new generation is composed of elite individuals and new individuals created by crossover and mutation operators.We iterate this process to maximize the fitness on a specific task and evolve desirable graph neural networks.
   æ–°ä¸€ä»£æ˜¯æ€ä¹ˆäº§ç”Ÿçš„ï¼Œè¿­ä»£çš„è¿è¡Œ
### æ€»ç»“
1. æå‡ºä¸¤ç§ï¼ˆä¸¤ä¸ªå±‚é¢ï¼‰äº¤å‰ç­–ç•¥ã€‚
2. ä½¿ç”¨ä¸¤ä¸ªå±‚é¢çš„äº¤å‰æ“ä½œ
3. å¯¹é€‚åº”å€¼æœ€ä¸å¥½çš„ä¸¤ä¸ªä¸ªä½“å˜å¼‚
### ç¼ºç‚¹
1. è¯„ä¼°èŠ±é”€å¤§--ç»“è®ºå¤„è¯´æ˜äº†è¿™ä¸€ç‚¹

##  2021-DE-GCN: Differential Evolution as an optimization algorithm for Graph Convolutional Networks
### å¥½è¡¨è¾¾
1. Neural networks had impressive results in recent years. Although neural networks only performed using Euclidean data in past decades, many data-sets in the real world have graph structures.This gap led researchers to implement deep learning on graphs.
   ç¥ç»ç½‘ç»œæœ€è¿‘å¾ˆæˆåŠŸï¼Œå¤¸èµå›¾
2. In recent years, learning with graphs and extracting latent information from networks is a hot research topic [1].
   è¿‘å¹´æ¥ï¼Œå›¾ç¥ç»ç½‘ç»œå¾ˆç«
3. Graphsâ€™ data structure is more complicated than the Euclidian data structure. As a result, this leads to complicated learning process in graphs. Authors in [2] have formulated neural networks for graphs.
   ä»‹ç»å›¾å¤æ‚çš„ç‰¹ç‚¹

## è‡ªå·±è®ºæ–‡
1. å› ä¸ºæ˜¯ç¬¬ä¸€æ¬¡ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œåšå…¥ä¾µæ£€æµ‹é¢†åŸŸçš„ç ”ç©¶ï¼Œåˆå§‹ç§ç¾¤æ‰¾ä¸åˆ°æˆåŠŸçš„æ¡ˆä¾‹ï¼Œæ‰€ä»¥ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ–¹æ³•åˆå§‹åŒ–åˆå§‹ç§ç¾¤ã€‚2021-Multiobjective Evolutionary
2. å¦‚æœä¸¤ä¸ªäº²æœ¬éƒ½æ˜¯æ¥è‡ªäºå¥½çš„ä¸ªä½“ï¼Œé‚£ä¹ˆå°±ä¼šåå¤åˆ©ç”¨å¥½çš„ä¸ªä½“çš„ç»„ä»¶ï¼Œä¼šä¸ä¼šå¯¼è‡´æ—©ç†Ÿå¹¶ä¸”ä¸¢å¤±å¤šæ ·æ€§ã€‚2021-Multiobjective Evolutionary
3. 2022-Evolutionary Searchå’Œ2021-Multiobjective Evolutionaryéƒ½æå‡ºäº†æ–°çš„äº¤å‰æ–¹å¼ï¼Œ2022æ˜¯äº¤å‰ç»“æ„åŸºå› æˆ–è€…switchåŸºå› ï¼Œ2021æ˜¯å—ï¼ˆreductionå—å’Œnormalå—ï¼‰äº¤å‰æˆ–è€…èŠ‚ç‚¹äº¤å‰
4. å½“æ¨¡å‹è®­ç»ƒåˆ°ä¸€å®šç¨‹åº¦ï¼Œå¯ä»¥ä½¿ç”¨æ¨¡å‹æ¥é‡‡æ ·ä¸ªä½“ï¼ˆå…ˆéšæœºé‡‡æ ·æ“ä½œç±»å‹ï¼Œç„¶åä½¿ç”¨æ¨¡å‹æ¥ç¡®å®šæ“ä½œçš„å…ˆåé¡ºåºç”Ÿæˆä¸ªä½“ï¼‰2021-Multiobjective Evolutionary è‡ªå·±å¯ä»¥åŠ å…¥åŒå­˜æ¡£ï¼Œè®ºæ–‡ä¸­æ¨¡å‹åªæ˜¯ä»å¥½çš„ç»„ä»¶å¾—åˆ°è´å¶æ–¯é‡‡æ ·æ¨¡å‹ï¼Œæ˜¯å¦å¯ä»¥ä½¿ç”¨åŒå­˜æ¡£æ¥è®­ç»ƒè´å¶æ–¯ï¼ˆåŒè´å¶æ–¯ï¼‰ã€‚
5. æœ‰residualï¼Œä½†æ˜¯ä¸æ˜¯ä»»æ„çš„residual 2022-GNN-EA: Graph Neural Network with Evolutionary  Algorithm